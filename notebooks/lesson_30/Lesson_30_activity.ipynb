{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783472ac",
   "metadata": {},
   "source": [
    "# Lesson 30 activity: hyperparameter optimization\n",
    "\n",
    "In the demo, we used Optuna to optimize just 2 hyperparameters:\n",
    "1. Number of convolutional blocks (1-4)\n",
    "2. Dropout rate (0.2-0.5)\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "Extend the optimization to include **additional hyperparameters**. The goal is to explore how different aspects of the model and training process affect performance.\n",
    "\n",
    "### Things to Consider\n",
    "\n",
    "Think about the different \"knobs\" you can tune in a neural network:\n",
    "\n",
    "- **Architecture choices**: How wide should the layers be? How many neurons in the fully connected layers?\n",
    "- **Training dynamics**: What about the learning rate? Could the optimizer itself be a choice?\n",
    "- **Regularization**: Are there other regularization techniques besides dropout?\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Look at Optuna's `suggest_*` methods: `suggest_int()`, `suggest_float()`, `suggest_categorical()`\n",
    "- Some hyperparameters might need to be passed to `create_cnn()`, while others might be used when creating the optimizer\n",
    "- Be careful about search space sizes - more hyperparameters means more trials needed!\n",
    "- Consider using `suggest_float(..., log=True)` for hyperparameters that span orders of magnitude\n",
    "\n",
    "### Suggested Starting Points (pick 1-2 to add)\n",
    "\n",
    "1. **Learning rate**: What range makes sense? (Hint: think logarithmic scale)\n",
    "2. **Initial filters**: The demo uses 32 - what if you tried 16, 32, or 64?\n",
    "3. **FC layer sizes**: Could you optimize the fully connected layer dimensions?\n",
    "4. **Optimizer choice**: Adam vs SGD vs RMSprop - which works best?\n",
    "5. **Batch size**: Does this affect final accuracy?\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "To run this notebook install `cifar10_tools` via pip: `pip install cifar10_tools`\n",
    "\n",
    "**Note**: If you are not working in one of the course deeplearning containers, you will also need to pip install `optuna` and `optuna-dashboard` to run this notebook.\n",
    "\n",
    "```\n",
    "pip install optuna optuna-dashboard\n",
    "```\n",
    "\n",
    "The Optuna dashboard can be viewed either via the VS Code extension 'Optuna Dashboard', or via the built-in web server. Start it with:\n",
    "\n",
    "```\n",
    "optuna-dashboard sqlite:///data/simple_optimization.db --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658b7b8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70d6c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom package imports\n",
    "from cifar10_tools.pytorch.training import train_model\n",
    "from cifar10_tools.pytorch.evaluation import evaluate_model\n",
    "from cifar10_tools.pytorch.plotting import (\n",
    "    plot_sample_images,\n",
    "    plot_learning_curves,\n",
    "    plot_confusion_matrix\n",
    ")\n",
    "\n",
    "# Suppress Optuna info messages (show only warnings and errors)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2444178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\Scripts\\python.exe\n",
      "Torch: 2.10.0+cpu\n",
      "CUDA build: None\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA build:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301676f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed hyperparameters (some of these could become tunable!)\n",
    "batch_size = 1000\n",
    "initial_filters = 32\n",
    "fc_units_1 = 512\n",
    "fc_units_2 = 128\n",
    "use_batch_norm = True\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Optuna settings\n",
    "n_trials = 30            # You may want more trials with more hyperparameters\n",
    "n_epochs_per_trial = 20  # Short training per trial\n",
    "n_epochs_final = 50      # Longer training for final model\n",
    "\n",
    "# Storage path for Optuna study - use a NEW name for your experiment!\n",
    "storage_path = Path('../data/activity_optimization.db')\n",
    "storage_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "storage_url = f'sqlite:///{storage_path.resolve()}'\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4dcd6",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3ba5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Data directory\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Transform: convert to tensor and normalize RGB channels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab77bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([40000, 3, 32, 32])\n",
      "X_val: torch.Size([10000, 3, 32, 32])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Preload data to GPU for faster training\n",
    "X_train_full = torch.stack([img for img, _ in train_dataset]).to(device)\n",
    "y_train_full = torch.tensor([label for _, label in train_dataset]).to(device)\n",
    "X_test = torch.stack([img for img, _ in test_dataset]).to(device)\n",
    "y_test = torch.tensor([label for _, label in test_dataset]).to(device)\n",
    "\n",
    "# Split training data into train and validation sets (80/20)\n",
    "n_train = int(0.8 * len(X_train_full))\n",
    "indices = torch.randperm(len(X_train_full))\n",
    "\n",
    "X_train = X_train_full[indices[:n_train]]\n",
    "y_train = y_train_full[indices[:n_train]]\n",
    "X_val = X_train_full[indices[n_train:]]\n",
    "y_val = y_train_full[indices[n_train:]]\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_val: {X_val.shape}')\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a97db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 40\n",
      "Validation batches: 10\n",
      "Test batches: 10\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_tensor_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_tensor_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_tensor_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Training batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')\n",
    "print(f'Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627967c",
   "metadata": {},
   "source": [
    "## 2. Define CNN Architecture\n",
    "\n",
    "**TODO**: Consider modifying this function to accept additional parameters.\n",
    "\n",
    "For example, you might want to make `initial_filters`, `fc_units_1`, or `fc_units_2` configurable.\n",
    "\n",
    "*Think about: What would need to change in the function signature and body?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4931ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(n_conv_blocks: int, dropout_rate: float, initial_filters: int) -> nn.Sequential:\n",
    "    '''Create a CNN with configurable architecture.\n",
    "    \n",
    "    Args:\n",
    "        n_conv_blocks: Number of convolutional blocks (1-4)\n",
    "        dropout_rate: Dropout probability\n",
    "        \n",
    "        # TODO: Add more parameters here if needed!\n",
    "        initial_filters: Initial Filter channels\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential model\n",
    "    '''\n",
    "\n",
    "    layers = []\n",
    "    in_channels = 3  # RGB input\n",
    "    current_size = 32  # Input image size\n",
    "    \n",
    "    for block_idx in range(n_conv_blocks):\n",
    "        out_channels = initial_filters * (2 ** block_idx)\n",
    "        \n",
    "        # Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> ReLU -> Pool -> Dropout\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.MaxPool2d(2, 2))\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        current_size //= 2\n",
    "    \n",
    "    # Calculate flattened size\n",
    "    final_channels = initial_filters * (2 ** (n_conv_blocks - 1))\n",
    "    flattened_size = final_channels * current_size * current_size\n",
    "    \n",
    "    # Classifier (3 fully connected layers)\n",
    "    layers.append(nn.Flatten())\n",
    "    layers.append(nn.Linear(flattened_size, fc_units_1))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(dropout_rate))\n",
    "    layers.append(nn.Linear(fc_units_1, fc_units_2))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(dropout_rate))\n",
    "    layers.append(nn.Linear(fc_units_2, num_classes))\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba1a3e",
   "metadata": {},
   "source": [
    "## 3. Optuna Hyperparameter Optimization\n",
    "\n",
    "### 3.1. Training Function for Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f363238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trial(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    n_epochs: int,\n",
    "    trial: optuna.Trial\n",
    ") -> float:\n",
    "    '''Train a model for a single Optuna trial with pruning support.'''\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "        \n",
    "        # Report for pruning\n",
    "        trial.report(val_accuracy, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38da50d",
   "metadata": {},
   "source": [
    "### 3.2. Define Objective Function\n",
    "\n",
    "**TODO**: This is where you add your new hyperparameters!\n",
    "\n",
    "Currently we optimize:\n",
    "- `n_conv_blocks`: 1 to 4 convolutional blocks\n",
    "- `dropout_rate`: 0.2 to 0.5\n",
    "\n",
    "**Your task**: Add at least 1-2 more hyperparameters to optimize.\n",
    "\n",
    "#### Useful Optuna methods:\n",
    "```python\n",
    "# For integers (e.g., number of filters, layer sizes)\n",
    "trial.suggest_int('param_name', low, high)\n",
    "\n",
    "# For floats (e.g., dropout, learning rate)\n",
    "trial.suggest_float('param_name', low, high)\n",
    "\n",
    "# For floats on log scale (great for learning rates!)\n",
    "trial.suggest_float('param_name', low, high, log=True)\n",
    "\n",
    "# For categorical choices (e.g., optimizer type)\n",
    "trial.suggest_categorical('param_name', ['option1', 'option2', 'option3'])\n",
    "```\n",
    "\n",
    "#### Example additions you might try:\n",
    "```python\n",
    "# Optimize learning rate (log scale is important here!)\n",
    "lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "# Optimize initial filter count\n",
    "init_filters = trial.suggest_categorical('initial_filters', [16, 32, 64])\n",
    "\n",
    "# Optimize optimizer choice\n",
    "optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8fe851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    '''Optuna objective function.\n",
    "    \n",
    "    TODO: Add more hyperparameters to optimize!\n",
    "    '''\n",
    "    \n",
    "    # === Existing hyperparameters ===\n",
    "    n_conv_blocks = trial.suggest_int('n_conv_blocks', 1, 4)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    \n",
    "    # === TODO: Add your new hyperparameters here! ===\n",
    "    # Example:\n",
    "    # lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    initial_filters = trial.suggest_categorical('initial_filters', [16,32])\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = create_cnn(\n",
    "        n_conv_blocks=n_conv_blocks,\n",
    "        dropout_rate=dropout_rate,\n",
    "        # TODO: Pass any new architecture parameters here\n",
    "        initial_filters=initial_filters\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    # TODO: If you're optimizing learning rate or optimizer type,\n",
    "    #       you'll need to modify this section!\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Set loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train and return validation accuracy\n",
    "    try:\n",
    "        return train_trial(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            n_epochs=n_epochs_per_trial,\n",
    "            trial=trial\n",
    "        )\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        raise optuna.TrialPruned('CUDA OOM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eabcbd",
   "metadata": {},
   "source": [
    "### 3.3. Run Optimization\n",
    "\n",
    "**Note**: With more hyperparameters, you may want to increase `n_trials` for better exploration of the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf876922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba97ef5d0fa4354b359c70d3d5bbc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2026-02-05 20:42:54,011]\u001b[0m Trial 6 failed with parameters: {'n_conv_blocks': 3, 'dropout_rate': 0.2969001859520436, 'initial_filters': 32} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\gahhh\\AppData\\Local\\Temp\\ipykernel_53116\\1478794227.py\", line 35, in objective\n",
      "    return train_trial(\n",
      "  File \"C:\\Users\\gahhh\\AppData\\Local\\Temp\\ipykernel_53116\\2358039236.py\", line 21, in train_trial\n",
      "    outputs = model(images)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 253, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\", line 73, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"c:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\functional.py\", line 1441, in dropout\n",
      "    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2026-02-05 20:42:54,033]\u001b[0m Trial 6 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Create Optuna study (maximize validation accuracy)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mstudy = optuna.create_study(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    direction=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    study_name=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactivity_optimization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    storage=storage_url,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    load_if_exists=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    pruner=optuna.pruners.MedianPruner(n_warmup_steps=3)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Run optimization\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mstudy.optimize(objective, n_trials=n_trials, show_progress_bar=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(f\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBest validation accuracy: \u001b[39;49m\u001b[38;5;132;43;01m{study.best_trial.value:.2f}\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(f\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mnBest hyperparameters:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfor key, value in study.best_trial.params.items():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(f\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;132;43;01m{key}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{value}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1370\u001b[0m, in \u001b[0;36mExecutionMagics.time\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1368\u001b[0m st \u001b[38;5;241m=\u001b[39m clock2()\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1370\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1371\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "File \u001b[1;32m<timed exec>:11\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\optuna\\study\\_optimize.py:68\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\optuna\\study\\_optimize.py:165\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\optuna\\study\\_optimize.py:263\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    259\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    262\u001b[0m ):\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\optuna\\study\\_optimize.py:206\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train and return validation accuracy\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mOutOfMemoryError:\n\u001b[0;32m     47\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mtrain_trial\u001b[1;34m(model, optimizer, criterion, train_loader, val_loader, n_epochs, trial)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\container.py:253\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gahhh\\source\\programming-basics\\programming_basics\\.venv310\\lib\\site-packages\\torch\\nn\\functional.py:1441\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1441\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1442\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Create Optuna study (maximize validation accuracy)\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='activity_optimization',\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True,\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=3)\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "print(f'Best validation accuracy: {study.best_trial.value:.2f}%')\n",
    "print(f'\\nBest hyperparameters:')\n",
    "\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed4c80",
   "metadata": {},
   "source": [
    "### 3.4. Visualize Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8658d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Optimization history\n",
    "axes[0].set_title('Optimization History')\n",
    "\n",
    "trial_numbers = [t.number for t in study.trials if t.value is not None]\n",
    "trial_values = [t.value for t in study.trials if t.value is not None]\n",
    "\n",
    "axes[0].plot(trial_numbers, trial_values, 'ko-', alpha=0.6)\n",
    "axes[0].axhline(y=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.2f}%')\n",
    "axes[0].set_xlabel('Trial')\n",
    "axes[0].set_ylabel('Validation Accuracy (%)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Hyperparameter importance\n",
    "axes[1].set_title('Hyperparameter Importance')\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "if len(completed_trials) >= 5:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    params = list(importance.keys())\n",
    "    values = list(importance.values())\n",
    "    axes[1].barh(params, values, color='steelblue')\n",
    "    axes[1].set_xlabel('Importance')\n",
    "\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Not enough trials\\nfor importance analysis', \n",
    "                 ha='center', va='center', transform=axes[1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c604a",
   "metadata": {},
   "source": [
    "## 4. Train Final Model with Best Hyperparameters\n",
    "\n",
    "**TODO**: Update this section to use any new hyperparameters you added!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "for key, value in best_params.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "# TODO: Pass any new parameters you added!\n",
    "best_model = create_cnn(\n",
    "    n_conv_blocks=best_params['n_conv_blocks'],\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer\n",
    "# TODO: Use best learning rate / optimizer if you optimized those!\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train for more epochs\n",
    "history = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=best_optimizer,\n",
    "    epochs=n_epochs_final,\n",
    "    print_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plot_learning_curves(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59457bf4",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, predictions, true_labels = evaluate_model(best_model, test_loader)\n",
    "print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig, ax = plot_confusion_matrix(true_labels, predictions, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3aa8db",
   "metadata": {},
   "source": [
    "## 6. Reflection Questions\n",
    "\n",
    "After completing your optimization, answer these questions:\n",
    "\n",
    "1. **Which hyperparameters did you add?** Why did you choose those?\n",
    "\n",
    "2. **Which hyperparameter was most important** according to Optuna's importance analysis?\n",
    "\n",
    "3. **Did adding more hyperparameters improve your best accuracy** compared to the demo's 2-parameter search?\n",
    "\n",
    "4. **What challenges did you encounter** when expanding the search space?\n",
    "\n",
    "5. **If you had more time/compute, what else would you try?**\n",
    "\n",
    "*Your answers here:*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
